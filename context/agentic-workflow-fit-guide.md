# Why AI Workflows Click for Some People and Not Others

![Three patterns of cognitive movement — linear, exploratory, and iterative — coexisting in the same space](../../../../data/images/generated/xai-1771202240780-2d2gfo.jpg)

It's not about intelligence. It's not about technical skill. It's not even about how much time you've spent with the tools.

I've been deep in agentic AI workflows for the past several weeks — building personal infrastructure, running agents, shipping features at a pace I've never hit before. And I've been watching other people try to adopt similar workflows and struggle. Not because they're less capable. Because the way they're approaching the tools doesn't match how their mind actually works.

That mismatch is the problem. And it's fixable — but not by trying harder with the same approach.

---

## The Observation

Some people sit down with an AI assistant and immediately start getting value. Others follow the same tutorials, read the same guides, and feel like they're fighting the tool the entire time. The common assumption is that the first group is just better at prompting. That's not what I've seen.

What I've seen is that the people who take to it quickly have — usually by accident — landed on a workflow that aligns with how they naturally think. And the people who struggle are trying to use the tool in a way that fights their cognitive patterns.

This isn't mystical. It's the same reason some developers thrive with TDD and others find it suffocating. Same reason some people love pair programming and others need to think alone first. The methodology isn't wrong. The fit is wrong.

This has been studied, by the way. Researchers have shown empirically that performance improves when the way information is presented matches the way a person processes it — and that technology improves individual performance when its capabilities match the tasks and the person using it. Neither finding is new. But applying them to how individuals adopt AI workflows — that's the conversation I think we should be having.

---

## Some Patterns I've Noticed

These aren't personality types. They're not categories you get sorted into. They're patterns I've observed in how people approach problems — and most of us are a blend. I'm sure there are more than what I've listed here. But recognizing which pattern dominates for you in a given context changes which AI workflow you should try first.

**The Linear Executor.** You think in sequences. Step one, then step two, then step three. You want to know the plan before you start. AI assistants feel chaotic to you — they jump around, suggest things out of order, lose track of where you were.

What works: give the AI a structured plan up front. Number your steps. Ask it to work through them in order and check in with you at each stage. Don't let it free-associate — constrain it to your sequence. You're not bad at AI. The default interaction mode just doesn't match your processing style.

**The Explorer.** You think by wandering. You ask a question, the answer sparks a tangent, the tangent reveals the real question you should have asked. AI assistants feel amazing to you — until you realize you've been chatting for an hour and have nothing concrete to show for it.

What works: set a timer or a scope before you start. "I'm exploring for 20 minutes, then I'm picking one thing to build." The exploration is genuinely valuable — it's how you find the right problem. You just need a forcing function to transition from exploration to execution. The AI won't do that for you.

**The Verifier.** You don't trust output you didn't produce yourself. AI-generated code feels foreign, even when it's correct. You spend as long reviewing as it would have taken to write it, and you're not sure you're saving time.

What works: start with the review, not the generation. Ask the AI to review *your* code first. Or ask it to write tests for your code. Let it prove itself as a reader before you trust it as a writer. Once you've seen it catch things you missed, the trust builds naturally. You're not resisting AI — you're applying the same quality standards you apply to everything else. That's an asset. You just need an entry point that respects it.

These are the patterns I've seen most clearly. There are almost certainly others — someone who needs to understand the entire system before touching any part of it, someone who's comfortable delegating but doesn't know what to ask for, someone who thinks by teaching and needs to explain the problem to the AI before they can solve it themselves. If you recognize a pattern I haven't described, that's data. The point isn't the taxonomy — it's the recognition that the pattern exists and has design implications.

---

## The Underlying Idea

The linear executor isn't wrong for wanting structure. The explorer isn't wrong for wanting to wander. The verifier isn't wrong for wanting proof. They just need different entry points into the same set of tools. Look at the image at the top of this document — three completely different movement patterns, all valid, all necessary, all part of the same system. The sum is greater than any individual path.

There's a well-established principle in cognitive science that your mind extends into the tools you use — that your notebook, your IDE, your AI assistant aren't just things you interact with, they're part of how you think. If that's true — and there's decades of research saying it is — then choosing the wrong tool interface isn't just inconvenient. It's like trying to think with the wrong part of your brain. The friction isn't laziness or resistance. It's architectural mismatch.

I spent a lot of time recently examining how my own mind actually works — not how I think it should work, but how it actually does. What sustains my focus. What breaks it. Where I get stuck and why. That examination changed how I use every tool I work with, not just AI. It drew on ideas from fields I didn't know existed when I started — cognitive work analysis, distributed cognition, metacognitive research. The principles are out there. They just haven't been widely applied to the question of "how should *I specifically* adopt AI tools?"

You don't need to do the same deep dive I did. But you might find it useful to ask yourself a few questions before you decide whether agentic workflows are "for you" or not.

---

## Five Questions Worth Sitting With

These aren't a quiz. There are no right answers. They're meant to surface something about how you work that might change which AI workflow you try next.

1. **When you get interrupted mid-task, how long does it take you to get back to where you were?** Seconds? Minutes? The rest of the afternoon? This tells you something about your context reconstruction cost — and whether you need an AI that preserves state for you or one that helps you rebuild it.

2. **Do you know what you want to build before you start, or do you figure it out by building?** Neither is wrong. But if you're a figure-it-out-by-building person using AI as a specification executor, you're going to hate it. Try using it as a thinking partner instead.

3. **When someone gives you feedback on your work, do you want to understand their reasoning or just know what to change?** This tells you whether you'll get more value from AI that explains its choices or AI that just produces output. Both exist. Most people default to the one that doesn't match.

4. **What's the last tool or process you abandoned, and why?** Not "it didn't work" — what specifically felt wrong? Was it too rigid? Too loose? Too much overhead? Too little structure? The friction pattern you find here probably applies to AI tools too.

5. **When do you do your best thinking — alone or in conversation?** If you think by talking, AI chat interfaces are a natural fit. If you think by writing, consider using AI as a reviewer rather than a generator. If you think by doing, skip the chat entirely and use AI-powered code completion or inline suggestions instead.

---

## The Bigger Picture

I'm not selling a framework here. I'm sharing an observation that's changed how I think about tool adoption in general: the reason something doesn't work for you usually isn't that you're doing it wrong. It's that the tool's assumptions about how you think don't match how you actually think.

That mismatch is diagnosable. And once you see it, the fix is usually obvious.

None of this is purely my invention. The relationship between cognitive patterns and tool effectiveness has been studied for decades — in cognitive psychology, human factors, human-computer interaction. What I'm trying to do is take those ideas — which mostly live in academic papers about safety-critical systems and organizational design — and apply them to something more immediate: how you and I adopt the tools that are changing our work right now.

If any of this resonated — if you recognized yourself in one of the patterns, or if one of the questions surfaced something you hadn't made explicit before — there's more behind it. I've been developing a methodology for this kind of self-examination that goes deeper than five questions. It's built on established cognitive science, applied in a way I haven't seen elsewhere. Happy to share it with anyone who's curious.

But the five questions are a fine place to start. You might be surprised what you find.

---

*Justin Heath — February 2026*
*This grew out of a personal project examining how cognitive patterns affect tool adoption. The verbose version exists. This is the one that respects your time.*
